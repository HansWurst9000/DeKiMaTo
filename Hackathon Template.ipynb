{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is structured as follows:\n",
    "1. Basics (Import libraries / Read in file)\n",
    "2. Clean dataset\n",
    "3. Optimise R^2\n",
    "4. Statistical Analysis\n",
    "5. Visualise results\n",
    "6. Save results\n",
    "7. Tipps and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BASICS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the relevant stuff in the beginning\n",
    "\n",
    "# basics\n",
    "import numpy as np\n",
    "\n",
    "# work with dataframes\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# statistical analysis\n",
    "import esda\n",
    "from esda.moran import Moran\n",
    "from splot.esda import moran_scatterplot, lisa_cluster, plot_local_autocorrelation\n",
    "\n",
    "import pysal as ps \n",
    "from pysal.lib import weights \n",
    "from libpysal.io import open as psopen \n",
    "import libpysal \n",
    "\n",
    "# data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# linear regression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .csv file\n",
    "file_name = 'data/Afghanistan.csv' # file path\n",
    "df = pd.read_csv(file_name)\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial information csv\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play around with parameters\n",
    "df = pd.read_csv(file_name, index_col=['id']) # set personalised index to column 'id'\n",
    "df = pd.read_csv(file_name, header=1) # first rows of dataset should be header\n",
    "df = pd.read_csv(file_name, skiprows=1) # skips first 5 rows\n",
    "df = pd.read_csv(file_name, skipfooter=1) # skips last row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign new column names\n",
    "df = df.columns = ['Column name 1', 'Column name 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in .shp file\n",
    "geo = gpd.read_file('data/IMD/lab04_imd.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial information shp\n",
    "geo.plot()\n",
    "geo.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CLEAN DATA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide dataset\n",
    "df = df[['Column 1', 'Column 2']]\n",
    "# or\n",
    "df = df.drop(['Column 1', 'Column 2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows \n",
    "df = df.drop(['Row 1', 'Row 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('column_name', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to reset the index\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values\n",
    "df.sort_values(by = ['column 1', 'column 2'], ascending = [False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data\n",
    "df.groupby('year').agg({'rating' : ['count','min', 'max']})\n",
    "# for each year in df, give the following attributes of rating: count, min , max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "pd.concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set empty values to NaN / 0\n",
    "df.dropna()  # Drop rows with missing values\n",
    "df.fillna(value=0, inplace=True)  # Replace NaNs with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***OPTIMISE R^2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train\n",
    "model_sm = sm.OLS(y_train, X_train).fit()\n",
    "print(model_sm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.drop(['col1', 'col2'], axis= 1)\n",
    "model_sm2 = sm.OLS(y_train, X_train2).fit()\n",
    "print(model_sm2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate to find best r^2\n",
    "X_train3 = X_train2.drop(['col3', 'col4'], axis= 1)\n",
    "model_sm3 = sm.OLS(y_train, X_train3).fit()\n",
    "print(model_sm3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STATISTICAL ANALYSIS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform linear regression\n",
    "# using statsmodel package (`statsmodels` needs us to explicitly add the constant by using `sm.add_constant`.)\n",
    "\n",
    "# 1. Import Libraries\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 2. load data\n",
    "df = pd.read_csv('data.csv', sep='\\t')  # Replace with your file path & tab-delimited\n",
    "\n",
    "# 3. explore data\n",
    "print(df.head())                        # Show the first few rows\n",
    "print(df.describe())                    # Summary statistics\n",
    "print(df.info())                        # Data types and non-null counts\n",
    "\n",
    "# 4. Handle Missing Values\n",
    "df = df.dropna()                        # Drop rows with missing values\n",
    "#or\n",
    "df.fillna(value=0, inplace=True)        # Replace NaNs with 0\n",
    "\n",
    "# 5. Split Data into Training and Test Sets\n",
    "X = df[['feature1', 'feature2']]        # Independent variables\n",
    "y = df['target']                        # Dependent variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Using Statsmodels for LR\n",
    "X_train = sm.add_constant(X_train)      # create the X matrix by appending a column of ones (constant) to x_train\n",
    "\n",
    "# build the OLS model \n",
    "model_sm = sm.OLS(y_train, X_train)\n",
    "\n",
    "# do the fit and save regression info in results_sm\n",
    "results_sm = model_sm.fit()\n",
    "\n",
    "X_test_sm = sm.add_constant(X_test)     # Predictions\n",
    "y_pred_sm = model_sm.predict(X_test_sm)\n",
    "\n",
    "# useful OLS info\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(results_sm.summary())\n",
    "\n",
    "# pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "# 6. Using Scikit-learn for LR (other option)\n",
    "model = LinearRegression()              # Create the model\n",
    "\n",
    "model.fit(X_train, y_train)             # Fit the model\n",
    "\n",
    "y_pred = model.predict(X_test)          # Predictions\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# 7. Visualize Results\n",
    "# Scatter plot and best-fit line\n",
    "plt.scatter(X_test['feature1'], y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test['feature1'], y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target Variable')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moran's i\n",
    "# 1. Define spatial weights\n",
    "w = weights.contiguity.Queen.from_dataframe(gdf)        # contiguity (queen or Rook for shared edges)\n",
    "\n",
    "w = weights.distance.KNN.from_dataframe(gdf, k=5)       # distance (using k-nearest neighbors)\n",
    "\n",
    "# 2. Select your variable of interest\n",
    "y = gdf['target_variable']\n",
    "\n",
    "# 3. Calculate Moran's I\n",
    "moran = esda.Moran(y, w)\n",
    "\n",
    "# 4. Print the result\n",
    "print(\"Moran's I:\", moran.I)\n",
    "print(\"P-value:\", moran.p_sim)\n",
    "print(\"Expected I:\", moran.EI)\n",
    "\n",
    "\"\"\" Moran’s I: The observed Moran’s I statistic. Positive values indicate clustering of similar values, and negative values suggest dispersion.\n",
    "P-value: The significance of the Moran’s I value. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. moran's plot\n",
    "fig, ax = moran_scatterplot(moran)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***VISUALISE RESULTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line diagram\n",
    "lineplot = sns.lineplot(df['Column name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot horizontal\n",
    "barh = sns.barplot(df['Column name'], orient = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot vertical\n",
    "barv = sns.barplot(df['Column name'], orient = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "scatterplot = sns.scatterplot(df['Column name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogramm\n",
    "histogram = sns.histplot(df['Column name'])\n",
    "histogram = sns.displot(df['Column name'])\n",
    "plt.xticks(rotation = -45, fontsize = 10) # Rotate labels by 45 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel density plot\n",
    "kde = sns.kdeplot(df['Column name'], fill = True)\n",
    "plt.xticks(rotation = -45, fontsize = 10) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot\n",
    "scatterplot = sns.boxplot(df['Column name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#violin plot\n",
    "violinplot = sns.violinplot(df['Column name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SAVE RESULTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "geo.to_csv('foldername/filename.csv', sep =',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as image\n",
    "plt.savefig('image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TIPPS AND TRICKS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print information\n",
    "print('Cell value of \"' + str(column_name) + '\" and \"' + str(row_name) + '\" is: ' + str(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
